{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4d3d50",
   "metadata": {},
   "source": [
    "# Tell a Joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bd985",
   "metadata": {},
   "source": [
    "Connect with Anthropic and Google llm APIs as well as OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84caaf86",
   "metadata": {},
   "source": [
    "Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18996f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"you are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted daddy joke for an audience working of engineers, QAs and product managers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43800fc9",
   "metadata": {},
   "source": [
    "## Tell a joke with Open AI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99655a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-mini\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-nano\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3-mini\n",
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f677b-5920-4c16-89f0-69ddb6eb8090",
   "metadata": {},
   "source": [
    "## Tell a Joke with Anthropic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with claude\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5daa9-80b3-4c60-8b8e-0aba310e9d17",
   "metadata": {},
   "source": [
    "## Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9495d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_prompt,\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
